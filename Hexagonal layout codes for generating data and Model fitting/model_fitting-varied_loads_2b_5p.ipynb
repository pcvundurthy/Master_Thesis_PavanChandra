{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_x_data\n",
      "read_y_data\n",
      "[[-0.97820564  0.29297193 -0.07061703 -0.96544182  0.2754832  -0.07559271\n",
      "  -0.95055615  0.26713403 -0.06478687 -0.94939434  0.28851273 -0.05467842\n",
      "  -1.02291172  0.31880814 -0.06668594 -1.02036267  0.29820167 -0.09116619\n",
      "  -0.98972578  0.26476729 -0.09036746 -0.9837491   0.30531915 -0.07307987\n",
      "  -1.06898421  0.34968716 -0.03087716 -1.12097493  0.35989869 -0.06120685\n",
      "  -1.15727766  0.31518646 -0.15790448 -1.05000599  0.3741363  -0.18206539\n",
      "  -0.59768246  0.20031284 -0.03087716 -0.54569173  0.19010131 -0.06120685\n",
      "  -0.50938901  0.23481354 -0.15790448 -0.61666068  0.1758637  -0.18206539]\n",
      " [-0.96984174  0.28045413 -0.07554428 -0.95366561  0.26681943 -0.06915842\n",
      "  -0.94867445  0.2803592  -0.05532045 -0.94855848  0.30831947 -0.06174094\n",
      "  -1.02365787  0.3062235  -0.08467621 -0.9992982   0.27119693 -0.09646884\n",
      "  -0.9827315   0.28755233 -0.07063151 -0.96914893  0.3312652  -0.09716293\n",
      "  -1.10428192  0.35754912 -0.04928571 -1.163557    0.34725352 -0.12266363\n",
      "  -1.09223439  0.34451607 -0.13466374 -0.93305379  0.33148657 -0.19556371\n",
      "  -0.56238474  0.19245088 -0.04928571 -0.50310967  0.20274648 -0.12266363\n",
      "  -0.57443228  0.20548393 -0.13466374 -0.73361288  0.21851343 -0.19556371]]\n",
      "[[ -35.    -35.      1.      0.      0.   -110.    -60.  ]\n",
      " [ -27.75  -35.      1.      0.      0.   -110.    -60.  ]]\n"
     ]
    }
   ],
   "source": [
    "#Reading the Training data (virtually generated) from a file \n",
    "#Input data\n",
    "x_data = pd.read_csv('x_rec_16_data_5.csv')\n",
    "x_data=x_data.values[:,1:]\n",
    "print(\"read_x_data\")\n",
    "#Output data\n",
    "y_data = pd.read_csv('y_rec_16_data_5.csv')\n",
    "y_data=y_data.values[:,1:]\n",
    "print(\"read_y_data\")\n",
    "print(x_data[:2,:])\n",
    "print(y_data[:2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11718, 48) (11718, 7) training data size\n",
      "(3907, 48) (3907, 7) Validation data size\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_data, y_data)\n",
    "print(np.shape(X_train), np.shape(y_train), \"training data size\")\n",
    "print(np.shape(X_val), np.shape(y_val),\"Validation data size\")\n",
    "#print(X_train[0:9,:])\n",
    "\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "scaler_x.fit(X_train)\n",
    "xtrain_scale=scaler_x.transform(X_train)\n",
    "\n",
    "scaler_x.fit(X_val)\n",
    "xval_scale=scaler_x.transform(X_val)\n",
    "\n",
    "scaler_y.fit(y_train)\n",
    "ytrain_scale=scaler_y.transform(y_train)\n",
    "\n",
    "scaler_y.fit(y_val)\n",
    "yval_scale=scaler_y.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_x_data\n",
      "read_y_data\n",
      "[[-0.97820564  0.29297193 -0.07061703 -0.96544182  0.2754832  -0.07559271\n",
      "  -0.95055615  0.26713403 -0.06478687 -0.94939434  0.28851273 -0.05467842\n",
      "  -1.02291172  0.31880814 -0.06668594 -1.02036267  0.29820167 -0.09116619\n",
      "  -0.98972578  0.26476729 -0.09036746 -0.9837491   0.30531915 -0.07307987\n",
      "  -1.06898421  0.34968716 -0.03087716 -1.12097493  0.35989869 -0.06120685\n",
      "  -1.15727766  0.31518646 -0.15790448 -1.05000599  0.3741363  -0.18206539\n",
      "  -0.59768246  0.20031284 -0.03087716 -0.54569173  0.19010131 -0.06120685\n",
      "  -0.50938901  0.23481354 -0.15790448 -0.61666068  0.1758637  -0.18206539]\n",
      " [-0.96984174  0.28045413 -0.07554428 -0.95366561  0.26681943 -0.06915842\n",
      "  -0.94867445  0.2803592  -0.05532045 -0.94855848  0.30831947 -0.06174094\n",
      "  -1.02365787  0.3062235  -0.08467621 -0.9992982   0.27119693 -0.09646884\n",
      "  -0.9827315   0.28755233 -0.07063151 -0.96914893  0.3312652  -0.09716293\n",
      "  -1.10428192  0.35754912 -0.04928571 -1.163557    0.34725352 -0.12266363\n",
      "  -1.09223439  0.34451607 -0.13466374 -0.93305379  0.33148657 -0.19556371\n",
      "  -0.56238474  0.19245088 -0.04928571 -0.50310967  0.20274648 -0.12266363\n",
      "  -0.57443228  0.20548393 -0.13466374 -0.73361288  0.21851343 -0.19556371]]\n",
      "[[ -35.    -35.      1.      0.      0.   -110.    -60.  ]\n",
      " [ -27.75  -35.      1.      0.      0.   -110.    -60.  ]]\n",
      "(11718, 48) (11718, 7) training data size\n",
      "(3907, 48) (3907, 7) Validation data size\n",
      "Model: \"sensor_concept_nn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 48)]              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 700)               34300     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 700)               490700    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 7)                 4907      \n",
      "=================================================================\n",
      "Total params: 529,907\n",
      "Trainable params: 529,907\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "367/367 - 2s - loss: 0.1923 - val_loss: 0.1609\n",
      "Epoch 2/30\n",
      "367/367 - 2s - loss: 0.1467 - val_loss: 0.1396\n",
      "Epoch 3/30\n",
      "367/367 - 2s - loss: 0.1299 - val_loss: 0.1216\n",
      "Epoch 4/30\n",
      "367/367 - 2s - loss: 0.1171 - val_loss: 0.1147\n",
      "Epoch 5/30\n",
      "367/367 - 2s - loss: 0.1104 - val_loss: 0.1132\n",
      "Epoch 6/30\n",
      "367/367 - 2s - loss: 0.1027 - val_loss: 0.1211\n",
      "Epoch 7/30\n",
      "367/367 - 2s - loss: 0.0943 - val_loss: 0.1044\n",
      "Epoch 8/30\n",
      "367/367 - 2s - loss: 0.0893 - val_loss: 0.0896\n",
      "Epoch 9/30\n",
      "367/367 - 2s - loss: 0.0831 - val_loss: 0.0885\n",
      "Epoch 10/30\n",
      "367/367 - 2s - loss: 0.0789 - val_loss: 0.0877\n",
      "Epoch 11/30\n",
      "367/367 - 2s - loss: 0.0751 - val_loss: 0.0857\n",
      "Epoch 12/30\n",
      "367/367 - 2s - loss: 0.0720 - val_loss: 0.0811\n",
      "Epoch 13/30\n",
      "367/367 - 2s - loss: 0.0696 - val_loss: 0.0789\n",
      "Epoch 14/30\n",
      "367/367 - 2s - loss: 0.0672 - val_loss: 0.0823\n",
      "Epoch 15/30\n",
      "367/367 - 2s - loss: 0.0646 - val_loss: 0.0784\n",
      "Epoch 16/30\n",
      "367/367 - 2s - loss: 0.0625 - val_loss: 0.0805\n",
      "Epoch 17/30\n",
      "367/367 - 2s - loss: 0.0612 - val_loss: 0.0785\n",
      "Epoch 18/30\n",
      "367/367 - 2s - loss: 0.0595 - val_loss: 0.0758\n",
      "Epoch 19/30\n",
      "367/367 - 2s - loss: 0.0575 - val_loss: 0.0806\n",
      "Epoch 20/30\n",
      "367/367 - 2s - loss: 0.0564 - val_loss: 0.0835\n",
      "Epoch 21/30\n",
      "367/367 - 2s - loss: 0.0553 - val_loss: 0.0793\n",
      "Epoch 22/30\n",
      "367/367 - 2s - loss: 0.0535 - val_loss: 0.0785\n",
      "Epoch 23/30\n",
      "367/367 - 2s - loss: 0.0529 - val_loss: 0.0766\n",
      "Epoch 24/30\n",
      "367/367 - 2s - loss: 0.0521 - val_loss: 0.0763\n",
      "Epoch 25/30\n",
      "367/367 - 2s - loss: 0.0510 - val_loss: 0.0753\n",
      "Epoch 26/30\n",
      "367/367 - 2s - loss: 0.0502 - val_loss: 0.0750\n",
      "Epoch 27/30\n",
      "367/367 - 2s - loss: 0.0494 - val_loss: 0.0777\n",
      "Epoch 28/30\n",
      "367/367 - 2s - loss: 0.0489 - val_loss: 0.0751\n",
      "Epoch 29/30\n",
      "367/367 - 2s - loss: 0.0483 - val_loss: 0.0774\n",
      "Epoch 30/30\n",
      "367/367 - 2s - loss: 0.0477 - val_loss: 0.0733\n"
     ]
    }
   ],
   "source": [
    "#Reading the Training data (virtually generated) from a file \n",
    "#Input data\n",
    "x_data = pd.read_csv('x_rec_16_data_5.csv')\n",
    "x_data=x_data.values[:,1:]\n",
    "print(\"read_x_data\")\n",
    "#Output data\n",
    "y_data = pd.read_csv('y_rec_16_data_5.csv')\n",
    "y_data=y_data.values[:,1:]\n",
    "print(\"read_y_data\")\n",
    "print(x_data[:2,:])\n",
    "print(y_data[:2,:])\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_data, y_data)\n",
    "print(np.shape(X_train), np.shape(y_train), \"training data size\")\n",
    "print(np.shape(X_val), np.shape(y_val),\"Validation data size\")\n",
    "#print(X_train[0:9,:])\n",
    "\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "scaler_x.fit(X_train)\n",
    "xtrain_scale=scaler_x.transform(X_train)\n",
    "\n",
    "scaler_x.fit(X_val)\n",
    "xval_scale=scaler_x.transform(X_val)\n",
    "\n",
    "scaler_y.fit(y_train)\n",
    "ytrain_scale=scaler_y.transform(y_train)\n",
    "\n",
    "scaler_y.fit(y_val)\n",
    "yval_scale=scaler_y.transform(y_val)\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from numpy import loadtxt\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "input_layer = Input(shape=(48,))\n",
    "hidden_layer_1 = Dense(units = 700, activation = 'relu')(input_layer)\n",
    "hidden_layer_2 = Dense(units = 700, activation = 'relu')(hidden_layer_1)\n",
    "output_layer = Dense(units = 7, activation = 'linear')(hidden_layer_2)\n",
    "model = Model(inputs = input_layer, outputs = output_layer, name = 'sensor_concept_nn')\n",
    "#opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "# define the keras model\n",
    "\n",
    "# Display the model\n",
    "model.summary()\n",
    "\n",
    "history_1 = model.fit(xtrain_scale, ytrain_scale, epochs=30, verbose=2, validation_data=(xval_scale,yval_scale), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_x_data\n",
      "read_y_data\n",
      "[[-0.97820564  0.29297193 -0.07061703 -0.96544182  0.2754832  -0.07559271\n",
      "  -0.95055615  0.26713403 -0.06478687 -0.94939434  0.28851273 -0.05467842\n",
      "  -1.02291172  0.31880814 -0.06668594 -1.02036267  0.29820167 -0.09116619\n",
      "  -0.98972578  0.26476729 -0.09036746 -0.9837491   0.30531915 -0.07307987\n",
      "  -1.06898421  0.34968716 -0.03087716 -1.12097493  0.35989869 -0.06120685\n",
      "  -1.15727766  0.31518646 -0.15790448 -1.05000599  0.3741363  -0.18206539\n",
      "  -0.59768246  0.20031284 -0.03087716 -0.54569173  0.19010131 -0.06120685\n",
      "  -0.50938901  0.23481354 -0.15790448 -0.61666068  0.1758637  -0.18206539]\n",
      " [-0.96984174  0.28045413 -0.07554428 -0.95366561  0.26681943 -0.06915842\n",
      "  -0.94867445  0.2803592  -0.05532045 -0.94855848  0.30831947 -0.06174094\n",
      "  -1.02365787  0.3062235  -0.08467621 -0.9992982   0.27119693 -0.09646884\n",
      "  -0.9827315   0.28755233 -0.07063151 -0.96914893  0.3312652  -0.09716293\n",
      "  -1.10428192  0.35754912 -0.04928571 -1.163557    0.34725352 -0.12266363\n",
      "  -1.09223439  0.34451607 -0.13466374 -0.93305379  0.33148657 -0.19556371\n",
      "  -0.56238474  0.19245088 -0.04928571 -0.50310967  0.20274648 -0.12266363\n",
      "  -0.57443228  0.20548393 -0.13466374 -0.73361288  0.21851343 -0.19556371]]\n",
      "[[ -35.    -35.      1.      0.      0.   -110.    -60.  ]\n",
      " [ -27.75  -35.      1.      0.      0.   -110.    -60.  ]]\n",
      "(11718, 48) (11718, 7) training data size\n",
      "(3907, 48) (3907, 7) Validation data size\n",
      "Model: \"sensor_concept_nn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 48)]              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 700)               34300     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 700)               490700    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 7)                 4907      \n",
      "=================================================================\n",
      "Total params: 529,907\n",
      "Trainable params: 529,907\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "367/367 - 2s - loss: 0.1925 - val_loss: 0.1674\n",
      "Epoch 2/30\n",
      "367/367 - 2s - loss: 0.1471 - val_loss: 0.1393\n",
      "Epoch 3/30\n",
      "367/367 - 2s - loss: 0.1296 - val_loss: 0.1208\n",
      "Epoch 4/30\n",
      "367/367 - 2s - loss: 0.1177 - val_loss: 0.1224\n",
      "Epoch 5/30\n",
      "367/367 - 2s - loss: 0.1109 - val_loss: 0.1181\n",
      "Epoch 6/30\n",
      "367/367 - 2s - loss: 0.1039 - val_loss: 0.1080\n",
      "Epoch 7/30\n",
      "367/367 - 2s - loss: 0.0973 - val_loss: 0.0922\n",
      "Epoch 8/30\n",
      "367/367 - 2s - loss: 0.0910 - val_loss: 0.1078\n",
      "Epoch 9/30\n",
      "367/367 - 2s - loss: 0.0884 - val_loss: 0.0896\n",
      "Epoch 10/30\n",
      "367/367 - 2s - loss: 0.0859 - val_loss: 0.0915\n",
      "Epoch 11/30\n",
      "367/367 - 2s - loss: 0.0828 - val_loss: 0.0891\n",
      "Epoch 12/30\n",
      "367/367 - 2s - loss: 0.0792 - val_loss: 0.0889\n",
      "Epoch 13/30\n",
      "367/367 - 2s - loss: 0.0778 - val_loss: 0.0859\n",
      "Epoch 14/30\n",
      "367/367 - 2s - loss: 0.0754 - val_loss: 0.0903\n",
      "Epoch 15/30\n",
      "367/367 - 2s - loss: 0.0750 - val_loss: 0.0891\n",
      "Epoch 16/30\n",
      "367/367 - 2s - loss: 0.0736 - val_loss: 0.0736\n",
      "Epoch 17/30\n",
      "367/367 - 2s - loss: 0.0713 - val_loss: 0.0855\n",
      "Epoch 18/30\n",
      "367/367 - 2s - loss: 0.0710 - val_loss: 0.0726\n",
      "Epoch 19/30\n",
      "367/367 - 2s - loss: 0.0696 - val_loss: 0.0874\n",
      "Epoch 20/30\n",
      "367/367 - 2s - loss: 0.0678 - val_loss: 0.0766\n",
      "Epoch 21/30\n",
      "367/367 - 2s - loss: 0.0674 - val_loss: 0.0741\n",
      "Epoch 22/30\n",
      "367/367 - 2s - loss: 0.0665 - val_loss: 0.0793\n",
      "Epoch 23/30\n",
      "367/367 - 2s - loss: 0.0660 - val_loss: 0.0755\n",
      "Epoch 24/30\n",
      "367/367 - 2s - loss: 0.0649 - val_loss: 0.0681\n",
      "Epoch 25/30\n",
      "367/367 - 2s - loss: 0.0638 - val_loss: 0.0872\n",
      "Epoch 26/30\n",
      "367/367 - 2s - loss: 0.0645 - val_loss: 0.0717\n",
      "Epoch 27/30\n",
      "367/367 - 2s - loss: 0.0615 - val_loss: 0.0734\n",
      "Epoch 28/30\n",
      "367/367 - 2s - loss: 0.0618 - val_loss: 0.0712\n",
      "Epoch 29/30\n",
      "367/367 - 2s - loss: 0.0599 - val_loss: 0.0739\n",
      "Epoch 30/30\n",
      "367/367 - 2s - loss: 0.0594 - val_loss: 0.0729\n"
     ]
    }
   ],
   "source": [
    "###for adam\n",
    "#Reading the Training data (virtually generated) from a file \n",
    "#Input data\n",
    "x_data = pd.read_csv('x_rec_16_data_5.csv')\n",
    "x_data=x_data.values[:,1:]\n",
    "print(\"read_x_data\")\n",
    "#Output data\n",
    "y_data = pd.read_csv('y_rec_16_data_5.csv')\n",
    "y_data=y_data.values[:,1:]\n",
    "print(\"read_y_data\")\n",
    "print(x_data[:2,:])\n",
    "print(y_data[:2,:])\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_data, y_data)\n",
    "print(np.shape(X_train), np.shape(y_train), \"training data size\")\n",
    "print(np.shape(X_val), np.shape(y_val),\"Validation data size\")\n",
    "#print(X_train[0:9,:])\n",
    "\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "scaler_x.fit(X_train)\n",
    "xtrain_scale=scaler_x.transform(X_train)\n",
    "\n",
    "scaler_x.fit(X_val)\n",
    "xval_scale=scaler_x.transform(X_val)\n",
    "\n",
    "scaler_y.fit(y_train)\n",
    "ytrain_scale=scaler_y.transform(y_train)\n",
    "\n",
    "scaler_y.fit(y_val)\n",
    "yval_scale=scaler_y.transform(y_val)\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from numpy import loadtxt\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "input_layer = Input(shape=(48,))\n",
    "hidden_layer_1 = Dense(units = 700, activation = 'relu')(input_layer)\n",
    "hidden_layer_2 = Dense(units = 700, activation = 'relu')(hidden_layer_1)\n",
    "output_layer = Dense(units = 7, activation = 'linear')(hidden_layer_2)\n",
    "model = Model(inputs = input_layer, outputs = output_layer, name = 'sensor_concept_nn')\n",
    "#opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "# define the keras model\n",
    "\n",
    "# Display the model\n",
    "model.summary()\n",
    "\n",
    "history_2 = model.fit(xtrain_scale, ytrain_scale, epochs=30, verbose=2, validation_data=(xval_scale,yval_scale))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to evaluate on training data\n",
    "#scores=model.evaluate(xtrain_scale,y_train_scale,verbose=0)\n",
    "#print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "#model.save(\"model.h5\")\n",
    "#print(\"saved the model to disk\")\n",
    "#to load model\n",
    "#model=load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "stress completed\n",
      "0\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "stress completed\n",
      "1\n",
      "(64, 48) shape of x_data\n",
      "(64, 7) shape of y_data\n"
     ]
    }
   ],
   "source": [
    "###Generate virtual training data \n",
    "##assign variables for crack coordinates, angle, stress intensity factors and T stress\n",
    "x0_all,y0_all,b0_all,K_1_all,K_2_all,T_all=([] for i in range(6))\n",
    "\n",
    "##Number of points and their range\n",
    "#Parameters in sequence:Stress Intensity Factor I, II, T-stress,angle & Coordinates of crack(y&x)\n",
    "#Number of points considered in each parameter = num_points\n",
    "num_points=2\n",
    "##Ranges for data parameters: \n",
    "#K_I=-300 to 1200; K_II=-110 to 130; T-stress=-60 to 22; angle=0 to 90; x_coordinate=-35 to 35; y_coordinate\n",
    "K_I_range =np.linspace(-400,1300,num_points)\n",
    "K_II_range=np.linspace(-140,150,num_points)\n",
    "T_range =np.linspace(-70,30,num_points)\n",
    "Beta_range =np.linspace(0,2*np.pi,num_points)\n",
    "Y_coord_range =np.linspace(-35,-6,num_points)\n",
    "X_coord_range =np.linspace(-35,-6,num_points)\n",
    "\n",
    "#Looping over to create data\n",
    "for a in K_I_range:\n",
    "    for b in K_II_range:\n",
    "        for c in T_range:\n",
    "            for angle in Beta_range:    \n",
    "                for j in Y_coord_range:    \n",
    "                    for i in X_coord_range:\n",
    "                        x0_all.append(i)        \n",
    "                        y0_all.append(j)\n",
    "                        b0_all.append(angle)\n",
    "                        K_1_all.append(a)\n",
    "                        K_2_all.append(b)\n",
    "                        T_all.append(c)\n",
    "                      \n",
    "#y_data = np.column_stack((x0_all, y0_all, b0_all))\n",
    "\n",
    "#print (x0_all)\n",
    "#print (y0_all)\n",
    "#print (b0_all)\n",
    "print (len(x0_all))\n",
    "\n",
    "##Obtaing strain field for varying x_0 and y_0 generated above\n",
    "count_2 = 0\n",
    "for count_1 in range(0,len(x0_all)):\n",
    "\t#from the local cordinate system and beta, let's obtain global coordinate system\n",
    "    x_0=x0_all[count_1]\n",
    "    y_0=y0_all[count_1]\n",
    "    beta = b0_all[count_1]\n",
    "    #Values of stress intensity factors extracted from the research journals Table 5: pure mode_I loading with different crack position\n",
    "\n",
    "    SIF_1=K_1_all[count_1]\n",
    "    SIF_2=K_2_all[count_1]\n",
    "    T=T_all[count_1]\n",
    "\n",
    "    \n",
    "    \n",
    "    hor_arr=4 #Horizontal arrangement of sensors\n",
    "    ver_arr=4 #vertical arrangement of sensors\n",
    "    num_sensors=hor_arr*ver_arr #number of sensors are regular rectangular arrangement \n",
    "    x_dash = np.zeros(num_sensors)\n",
    "    y_dash = np.zeros(num_sensors)\n",
    "\n",
    "    x = np.zeros(num_sensors)\n",
    "    y = np.zeros(num_sensors)\n",
    "    phi = np.zeros(num_sensors)\n",
    "    x_count = 0\n",
    "    y_count = 0\n",
    "    x_dist=10 #distance between sensors\n",
    "    y_dist=10    \n",
    "\n",
    "\n",
    "    #below is the snip for x_dash, y_dash coordinates w.r.t film C.S.\n",
    "    #np.add(y_dash,y_dist) \n",
    "    count = 0\n",
    "    for k in range(1,ver_arr+1):\n",
    "        for l in range(1,hor_arr+1):\n",
    "            x_dash[count]= 0+x_dist*l\n",
    "            y_dash[count]= 0+y_dist*k\n",
    "            count  = count+1   \n",
    "\n",
    "    #obtaining global coordinate values\n",
    "    for i in range(0,num_sensors):\n",
    "        x[i] = x_dash[i]*np.cos(beta)-y_dash[i]*np.sin(beta)+x_0\n",
    "        y[i] = y_dash[i]*np.sin(beta)+y_dash[i]*np.cos(beta)+y_0 \n",
    "\n",
    "\n",
    "    \n",
    "    #coordinates of the measuring points (r,phi)\n",
    "    r = np.sqrt(x**2+y**2)\n",
    "    phi = np.arctan2(y,x)\n",
    "\n",
    "    #Values of stress intensity factors extracted from the research journals Table 5: pure mode_I loading with different crack position\n",
    "    nu = 0.33\n",
    "    E = 72\n",
    "\n",
    "    #with warnings.catch_warnings():\n",
    "        #warnings.filterwarnings('error')\n",
    "        \n",
    "        \n",
    "    np.seterr(divide='raise')\n",
    "    try:\n",
    "        stress_11 = ((SIF_1/(np.sqrt(2*np.pi*r))) * (np.cos(phi/2)) * (1-np.sin(phi/2)*np.sin((3*phi)/2))) - ((SIF_2/(np.sqrt(2*np.pi*r))) * np.sin(phi/2) * (2+np.cos(phi/2)*np.cos(3*phi/2))) + T\n",
    "        stress_22 = ((SIF_1/(np.sqrt(2*np.pi*r))) * (np.cos(phi/2)) * (1+np.sin(phi/2)*np.sin((3*phi)/2))) + ((SIF_2/(np.sqrt(2*np.pi*r))) * np.sin(phi/2) * (np.cos(phi/2) * np.cos(3*phi/2)))\n",
    "        stress_12 = ((SIF_1/(np.sqrt(2*np.pi*r))) * (np.sin(phi/2)) * (np.cos(phi/2) * np.cos((3*phi)/2))) + ((SIF_2/(np.sqrt(2*np.pi*r))) * np.cos(phi/2) * (1-(np.sin(phi/2)*np.sin(3*phi/2))))\n",
    "        print(\"stress completed\")\n",
    "\n",
    "    except FloatingPointError:\n",
    "        #print(count_1)\n",
    "        continue\n",
    "    \n",
    "        \n",
    "    \n",
    "    #strains of the specimen\n",
    "    eps_specimen_11 = (stress_11-(nu*stress_22))/E\n",
    "    eps_specimen_22 = (stress_22-(nu*stress_11))/E\n",
    "    eps_specimen_12 = (1+nu)*stress_12/E\n",
    "    print(count_2)\n",
    "\n",
    "    #combining all the individual arrays for the sensors placed and forming a single array \n",
    "    stack_arr = np.column_stack((eps_specimen_11,eps_specimen_22,eps_specimen_12))\n",
    "    xgroup = stack_arr.flatten()\n",
    "    ygroup = np.column_stack((x_0,y_0,np.cos(beta),np.sin(beta),SIF_1,SIF_2,T))\n",
    "\n",
    "\n",
    "    if (count_2 == 0):\n",
    "        final_xgroup = xgroup\n",
    "        final_ygroup = ygroup\n",
    "        count_2 = 1\n",
    "    else:\n",
    "        final_xgroup = np.vstack((final_xgroup, xgroup))\n",
    "        final_ygroup = np.vstack((final_ygroup, ygroup))\n",
    "        \n",
    "x_add_data = final_xgroup\n",
    "y_add_data = final_ygroup\n",
    "\n",
    "print (np.shape(x_add_data),\"shape of x_data\") #16 sensors and 5 outputs for each sensor, so 80\n",
    "print(np.shape(y_add_data),\"shape of y_data\") # crack location(x and y) and angle(so 3 columns)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-33045f8793a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_add_scale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaler_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_add_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mscores_for_case_1_lr\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mhistory_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_add_scale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_add_scale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mscores_for_case_1_adam\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mhistory_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_add_scale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_add_scale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'History' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "scaler_x.fit(x_add_data)\n",
    "x_add_scale=scaler_x.transform(x_add_data)\n",
    "\n",
    "scaler_y.fit(y_add_data)\n",
    "y_add_scale=scaler_y.transform(y_add_data)\n",
    "\n",
    "scores_for_case_1_lr= history_1.evaluate(x_add_scale, y_add_scale, verbose=2)\n",
    "scores_for_case_1_adam= history_2.evaluate(x_add_scale, y_add_scale, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TRYING LEVELS OF LOADING OUTSIDE THE TRAINING REGION\n",
    "\n",
    "\n",
    "\n",
    "###Generate virtual training data \n",
    "##assign variables for crack coordinates, angle, stress intensity factors and T stress\n",
    "x0_all,y0_all,b0_all,K_1_all,K_2_all,T_all=([] for i in range(6))\n",
    "\n",
    "##Number of points and their range\n",
    "#Parameters in sequence:Stress Intensity Factor I, II, T-stress,angle & Coordinates of crack(y&x)\n",
    "#Number of points considered in each parameter = num_points\n",
    "num_points=2\n",
    "##Ranges for data parameters: \n",
    "#K_I=-300 to 1200; K_II=-110 to 130; T-stress=-60 to 22; angle=0 to 90; x_coordinate=-35 to 35; y_coordinate\n",
    "K_I_range =np.linspace(-600,1600,num_points)\n",
    "K_II_range=np.linspace(-200,200,num_points)\n",
    "T_range =np.linspace(-100,50,num_points)\n",
    "Beta_range =np.linspace(0,2*np.pi,num_points)\n",
    "Y_coord_range =np.linspace(-35,-6,num_points)\n",
    "X_coord_range =np.linspace(-35,-6,num_points)\n",
    "\n",
    "#Looping over to create data\n",
    "for a in K_I_range:\n",
    "    for b in K_II_range:\n",
    "        for c in T_range:\n",
    "            for angle in Beta_range:    \n",
    "                for j in Y_coord_range:    \n",
    "                    for i in X_coord_range:\n",
    "                        x0_all.append(i)        \n",
    "                        y0_all.append(j)\n",
    "                        b0_all.append(angle)\n",
    "                        K_1_all.append(a)\n",
    "                        K_2_all.append(b)\n",
    "                        T_all.append(c)\n",
    "                      \n",
    "#y_data = np.column_stack((x0_all, y0_all, b0_all))\n",
    "\n",
    "#print (x0_all)\n",
    "#print (y0_all)\n",
    "#print (b0_all)\n",
    "print (len(x0_all))\n",
    "\n",
    "##Obtaing strain field for varying x_0 and y_0 generated above\n",
    "count_2 = 0\n",
    "for count_1 in range(0,len(x0_all)):\n",
    "\t#from the local cordinate system and beta, let's obtain global coordinate system\n",
    "    x_0=x0_all[count_1]\n",
    "    y_0=y0_all[count_1]\n",
    "    beta = b0_all[count_1]\n",
    "    #Values of stress intensity factors extracted from the research journals Table 5: pure mode_I loading with different crack position\n",
    "\n",
    "    SIF_1=K_1_all[count_1]\n",
    "    SIF_2=K_2_all[count_1]\n",
    "    T=T_all[count_1]\n",
    "\n",
    "    \n",
    "    \n",
    "    hor_arr=4 #Horizontal arrangement of sensors\n",
    "    ver_arr=4 #vertical arrangement of sensors\n",
    "    num_sensors=hor_arr*ver_arr #number of sensors are regular rectangular arrangement \n",
    "    x_dash = np.zeros(num_sensors)\n",
    "    y_dash = np.zeros(num_sensors)\n",
    "\n",
    "    x = np.zeros(num_sensors)\n",
    "    y = np.zeros(num_sensors)\n",
    "    phi = np.zeros(num_sensors)\n",
    "    x_count = 0\n",
    "    y_count = 0\n",
    "    x_dist=10 #distance between sensors\n",
    "    y_dist=10    \n",
    "\n",
    "\n",
    "    #below is the snip for x_dash, y_dash coordinates w.r.t film C.S.\n",
    "    #np.add(y_dash,y_dist) \n",
    "    count = 0\n",
    "    for k in range(1,ver_arr+1):\n",
    "        for l in range(1,hor_arr+1):\n",
    "            x_dash[count]= 0+x_dist*l\n",
    "            y_dash[count]= 0+y_dist*k\n",
    "            count  = count+1   \n",
    "\n",
    "    #obtaining global coordinate values\n",
    "    for i in range(0,num_sensors):\n",
    "        x[i] = x_dash[i]*np.cos(beta)-y_dash[i]*np.sin(beta)+x_0\n",
    "        y[i] = y_dash[i]*np.sin(beta)+y_dash[i]*np.cos(beta)+y_0 \n",
    "\n",
    "\n",
    "    \n",
    "    #coordinates of the measuring points (r,phi)\n",
    "    r = np.sqrt(x**2+y**2)\n",
    "    phi = np.arctan2(y,x)\n",
    "\n",
    "    #Values of stress intensity factors extracted from the research journals Table 5: pure mode_I loading with different crack position\n",
    "    nu = 0.33\n",
    "    E = 72\n",
    "\n",
    "    #with warnings.catch_warnings():\n",
    "        #warnings.filterwarnings('error')\n",
    "        \n",
    "        \n",
    "    np.seterr(divide='raise')\n",
    "    try:\n",
    "        stress_11 = ((SIF_1/(np.sqrt(2*np.pi*r))) * (np.cos(phi/2)) * (1-np.sin(phi/2)*np.sin((3*phi)/2))) - ((SIF_2/(np.sqrt(2*np.pi*r))) * np.sin(phi/2) * (2+np.cos(phi/2)*np.cos(3*phi/2))) + T\n",
    "        stress_22 = ((SIF_1/(np.sqrt(2*np.pi*r))) * (np.cos(phi/2)) * (1+np.sin(phi/2)*np.sin((3*phi)/2))) + ((SIF_2/(np.sqrt(2*np.pi*r))) * np.sin(phi/2) * (np.cos(phi/2) * np.cos(3*phi/2)))\n",
    "        stress_12 = ((SIF_1/(np.sqrt(2*np.pi*r))) * (np.sin(phi/2)) * (np.cos(phi/2) * np.cos((3*phi)/2))) + ((SIF_2/(np.sqrt(2*np.pi*r))) * np.cos(phi/2) * (1-(np.sin(phi/2)*np.sin(3*phi/2))))\n",
    "  \n",
    "\n",
    "    except FloatingPointError:\n",
    "        #print(count_1)\n",
    "        continue\n",
    "    \n",
    "        \n",
    "    \n",
    "    #strains of the specimen\n",
    "    eps_specimen_11 = (stress_11-(nu*stress_22))/E\n",
    "    eps_specimen_22 = (stress_22-(nu*stress_11))/E\n",
    "    eps_specimen_12 = (1+nu)*stress_12/E\n",
    "    #print(count_2)\n",
    "\n",
    "\n",
    "    #combining all the individual arrays for the sensors placed and forming a single array \n",
    "    stack_arr = np.column_stack((eps_specimen_11,eps_specimen_22,eps_specimen_12))\n",
    "    xgroup = stack_arr.flatten()\n",
    "    ygroup = np.column_stack((x_0,y_0,np.cos(beta),np.sin(beta),SIF_1,SIF_2,T))\n",
    "\n",
    "\n",
    "    if (count_2 == 0):\n",
    "        final_xgroup = xgroup\n",
    "        final_ygroup = ygroup\n",
    "        count_2 = 1\n",
    "    else:\n",
    "        final_xgroup = np.vstack((final_xgroup, xgroup))\n",
    "        final_ygroup = np.vstack((final_ygroup, ygroup))\n",
    "        \n",
    "x_add_data_2 = final_xgroup\n",
    "y_add_data_2 = final_ygroup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x.fit(x_add_data_2)\n",
    "x_add_scale_2=scaler_x.transform(x_add_data_2)\n",
    "\n",
    "scaler_y.fit(y_add_data_2)\n",
    "y_add_scale_2=scaler_y.transform(y_add_data_2)\n",
    "\n",
    "#scores_for_case_2= model.evaluate(x_add_scale_2, y_add_scale_2, verbose=2)\n",
    "scores_for_case_2_lr= history_1.evaluate(x_add_scale, y_add_scale, verbose=2)\n",
    "scores_for_case_2_adam= history_2.evaluate(x_add_scale, y_add_scale, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
